\cleardoublepage
\chapter{Results}
\label{ch:results}
\label{ch:chapter4}

\section{Retrieval Results}

We first evaluated the retrieval performance of our baseline systems introduced in Section \ref{baselines_sec}. We found that ColBERTV2 consistently demonstrated strong performance across all datasets. In contrast, HippoRAG showed particularly high retrieval performance on MuSiQue, which is attributed to the datasetâ€™s entity-centric design and also supported by the HippoRAG authors \cite{NEURIPS2024_6ddc001d}. However, HippoRAG underperformed on easier datasets relative to simpler methods like BM25. 

\noindent We also observe that MuSiQue and 2Wiki have the lowest overall retrieval scores. This is consistent with the design of these benchmarks, which introduce complexity through actual multi-hop reasoning.

\noindent Table \ref{tab:retrieval_results} summarizes retrieval performance across all systems, reporting \textbf{recall@k} for various values of $k$. Importantly, the vast majorify of questions in the datasets require no more than five passages as supporting evidence, which highlights the importance of high recall at low values of $k$ to reduce irrelevant content during the question answering phase.

\input{chapter4/retrieval_results}

\section{Question Answering Results}

\noindent Figure \ref{fig:scores_gpt4o} shows the results obtained using GPT-4o-mini across the four datasets using ColBERTV2, msmarco-bert-base-dot-v5, and BM25 retrievers. While performance generally improves with increasing $k$, the gains diminish as more irrelevant details begin to interfere with the model's ability to identify relevant information within its context.

\noindent Interestingly, a performance drop is observed on LoCoMo, HotpotQA, and 2Wiki under the full-context setup, which suggests that excessive context introduces noise. However, MuSiQue, the most complex dataset, benefits modestly from the added information.

\input{chapter4/scores_gpt4o}

\noindent We also evaluate Qwen2.5-14B using a slightly adapted prompt, see Appendix \ref{fig:qa-base-qwen}.  As shown in Figure \ref{fig:scores_qwen}, its performance closely matches that of GPT-4o-mini, demonstrating that smaller models with strong instruction tuning can achieve competitive results.

\input{chapter4/scores_qwen}

\noindent We also test o3-mini using $k = 5$. As expected, o3-mini matches or outperforms other models with higher $k$ values, suggesting that reasoning ability plays a key role for MHQA task.

\noindent Finally, we include QA results from HippoRAG \cite{NEURIPS2024_6ddc001d}, which incorporates graph-structured memory. HippoRAG performs especially well on the MuSiQue dataset, likely due to its ability to abstract relationships and reason over structured content.

\noindent Detailed results for all baseline configurations are provided in Table \ref{tab:qa_results}.

\input{chapter4/qa_results}
