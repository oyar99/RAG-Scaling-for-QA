@article{
    sumers2024cognitive,
    title={Cognitive Architectures for Language Agents},
    author={Theodore Sumers and Shunyu Yao and Karthik Narasimhan and Thomas Griffiths},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2024},
    url={https://openreview.net/forum?id=1i6ZCvflQJ},
}

@misc{minaee2024largelanguagemodelssurvey,
    title={Large Language Models: A Survey}, 
    author={Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
    year={2024},
    eprint={2402.06196},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2402.06196}, 
}

@article{liu2023lostmiddlelanguagemodels,
    author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
    address = {One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA},
    issn = {2307-387X},
    journal = {Transactions of the Association for Computational Linguistics},
    keywords = {Computer science ; Linguistics ; Social sciences ; Technology},
    language = {eng},
    abstract = {AbstractWhile recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
    pages = {157-173},
    publisher = {MIT Press},
    title = {Lost in the Middle: How Language Models Use Long Contexts},
    volume = {12},
    year = {2024},
}

@misc{openai2024gpt4technicalreport,
    title={GPT-4 Technical Report}, 
    author={OpenAI and others},
    year={2024},
    eprint={2303.08774},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2303.08774}, 
}

@misc{geminiteam2024gemini15unlockingmultimodal,
    title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
    author={Gemini Team and others},
    year={2024},
    eprint={2403.05530},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2403.05530}, 
}

@inproceedings{kitaev2020reformerefficienttransformer,
    author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
    title={Reformer: The Efficient Transformer},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2020},
    url={https://arxiv.org/abs/2001.04451},
    keywords={attention, locality sensitive hashing, reversible layers}
}

@inproceedings{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
    author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
    pages = {9459--9474},
    publisher = {Curran Associates, Inc.},
    title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
    url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
    volume = {33},
    year = {2020}
}

@inproceedings{NEURIPS2024_6ddc001d,
    author = {Jimenez Gutierrez, Bernal and Shu, Yiheng and Gu, Yu and Yasunaga, Michihiro and Su, Yu},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
    pages = {59532--59569},
    publisher = {Curran Associates, Inc.},
    title = {HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models},
    url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/6ddc001d07ca4f319af96a3024f6dbd1-Paper-Conference.pdf},
    volume = {37},
    year = {2024}
}

@misc{liang2024kagboostingllmsprofessional,
    title={KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation}, 
    author={Lei Liang and Mengshu Sun and Zhengke Gui and Zhongshu Zhu and Zhouyu Jiang and Ling Zhong and Yuan Qu and Peilong Zhao and Zhongpu Bo and Jin Yang and Huaidong Xiong and Lin Yuan and Jun Xu and Zaoyang Wang and Zhiqiang Zhang and Wen Zhang and Huajun Chen and Wenguang Chen and Jun Zhou},
    year={2024},
    eprint={2409.13731},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2409.13731}, 
}

@article{packer2024memgptllmsoperatingsystems,
    title={{MemGPT}: Towards LLMs as Operating Systems},
    author={Packer, Charles and Wooders, Sarah and Lin, Kevin and Fang, Vivian and Patil, Shishir G. and Stoica, Ion and Gonzalez, Joseph E.},
    journal={arXiv preprint arXiv:2310.08560},
    year={2024},
    eprint={2310.08560},
    archivePrefix={arXiv},
    primaryClass={cs.AI},
    url={https://arxiv.org/abs/2310.08560},
}

@inproceedings{li2024graphreaderbuildinggraphbasedagent,
    title={{G}raph{R}eader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models},
    author={Li, Shilong  and He, Yancheng  and Guo, Hangyu  and Bu, Xingyuan  and Bai, Ge  and Liu, Jie  and Liu, Jiaheng  and Qu, Xingwei  and Li, Yangguang  and Ouyang, Wanli  and Su, Wenbo  and Zheng, Bo},
    editor={Al-Onaizan, Yaser  and Bansal, Mohit  and Chen, Yun-Nung},
    booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
    month={nov},
    year={2024},
    address={Miami, Florida, USA},
    publisher={Association for Computational Linguistics},
    url={https://aclanthology.org/2024.findings-emnlp.746/},
    doi={10.18653/v1/2024.findings-emnlp.746},
    pages={12758--12786},
    abstract={Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. Despite numerous efforts made to optimize LLMs for long contexts, challenges persist in robustly processing long inputs. In this paper, we introduce GraphReader, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. It then invokes a set of predefined functions to read node content and neighbors, facilitating a coarse-to-fine exploration of the graph. Throughout the exploration, the agent continuously records new insights and reflects on current circumstances to optimize the process until it has gathered sufficient information to generate an answer. Experimental results on the LV-Eval dataset reveal that GraphReader using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin. Additionally, our approach demonstrates superior performance on four challenging single-hop and multi-hop benchmarks.}
}

@misc{anokhin2024arigraphlearningknowledgegraph,
    title={AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents}, 
    author={Petr Anokhin and Nikita Semenov and Artyom Sorokin and Dmitry Evseev and Mikhail Burtsev and Evgeny Burnaev},
    year={2024},
    eprint={2407.04363},
    archivePrefix={arXiv},
    primaryClass={cs.AI},
    url={https://arxiv.org/abs/2407.04363}, 
}

@inproceedings{zhao-etal-2024-longagent,
    title = "{LONGAGENT}: Achieving Question Answering for 128k-Token-Long Documents through Multi-Agent Collaboration",
    author = "Zhao, Jun  and
      Zu, Can  and
      Hao, Xu  and
      Lu, Yi  and
      He, Wei  and
      Ding, Yiwen  and
      Gui, Tao  and
      Zhang, Qi  and
      Huang, Xuanjing",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.912/",
    doi = "10.18653/v1/2024.emnlp-main.912",
    pages = "16310--16324",
    abstract = "Large language models (LLMs) have achieved tremendous success in understanding language and processing text. However, question-answering (QA) on lengthy documents faces challenges of resource constraints and a high propensity for errors, even for the most advanced models such as GPT-4 and Claude2.In this paper, we introduce {\_}LongAgent{\_}, a multi-agent collaboration method that enables efficient and effective QA over $128k$-token-long documents. {\_}LongAgent{\_} adopts a {\_}divide-and-conquer{\_} strategy, breaking down lengthy documents into shorter, more manageable text chunks. A leader agent comprehends the user`s query and organizes the member agents to read their assigned chunks, reasoning a final answer through multiple rounds of discussion.Due to members' hallucinations, it`s difficult to guarantee that every response provided by each member is accurate.To address this, we develop an {\_}inter-member communication{\_} mechanism that facilitates information sharing, allowing for the detection and mitigation of hallucinatory responses.Experimental results show that a LLaMA-2 7B driven by {\_}LongAgent{\_} can effectively support QA over $128k$-token documents, achieving 16.42{\%} and 1.63{\%} accuracy gains over GPT-4 on single-hop and multi-hop QA settings, respectively."
}

@article{10.1561/1500000102,
    author = {Mavi, Vaibhav and Jangra, Anubhav and Jatowt, Adam},
    title = {Multi-hop Question Answering},
    year = {2024},
    issue_date = {Jun 2024},
    publisher = {Now Publishers Inc.},
    address = {Hanover, MA, USA},
    volume = {17},
    number = {5},
    issn = {1554-0669},
    url = {https://doi.org/10.1561/1500000102},
    doi = {10.1561/1500000102},
    abstract = {The task of Question Answering (QA) has attracted significant
    research interest for a long time. Its relevance to
    language understanding and knowledge retrieval tasks, along
    with the simple setting, makes the task of QA crucial for
    strong AI systems. Recent success on simple QA tasks has
    shifted the focus to more complex settings. Among these,
    Multi-Hop QA (MHQA) is one of the most researched tasks
    over recent years. In broad terms, MHQA is the task of answering
    natural language questions that involve extracting
    and combining multiple pieces of information and doing multiple
    steps of reasoning. An example of a multi-hop question
    would be “The Argentine PGA Championship record holder
    has won how many tournaments worldwide?”. Answering
    the question would need two pieces of information: “Who is
    the record holder for Argentine PGA Championship tournaments?”
    and “How many tournaments did [Answer of Sub
    Q1] win?”. The ability to answer multi-hop questions and
    perform multi step reasoning can significantly improve the
    utility of NLP systems. Consequently, the field has seen a
    surge of high quality datasets, models and evaluation strategies.
    The notion of ‘multiple hops’ is somewhat abstract
    which results in a large variety of tasks that require multihop
    reasoning. This leads to different datasets and models
    that differ significantly from each other and make the field
    challenging to generalize and survey. We aim to provide a
    general and formal definition of the MHQA task, and organize
    and summarize existing MHQA frameworks. We also
    outline some best practices for building MHQA datasets.
    This monograph provides a systematic and thorough introduction
    as well as the structuring of the existing attempts
    to this highly interesting, yet quite challenging task.},
    journal = {Found. Trends Inf. Retr.},
    month = jun,
    pages = {457–586},
    numpages = {133}
}

@inproceedings{maharana-etal-2024-evaluating,
    title={Evaluating Very Long-Term Conversational Memory of {LLM} Agents},
    author={Maharana, Adyasha  and
        Lee, Dong-Ho  and
        Tulyakov, Sergey  and
        Bansal, Mohit  and
        Barbieri, Francesco  and
        Fang, Yuwei},
    editor={Ku, Lun-Wei  and
        Martins, Andre  and
        Srikumar, Vivek},
    booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month={aug},
    year={2024},
    address={Bangkok, Thailand},
    publisher={Association for Computational Linguistics},
    url={https://aclanthology.org/2024.acl-long.747/},
    doi={10.18653/v1/2024.acl-long.747},
    pages={13851--13870},
    abstract={Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 600 turns and 16K tokens on avg., over up to 32 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.}
}

@inproceedings{yang2018hotpotqa,
    title={{HotpotQA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},
    author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
    booktitle={Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
    year={2018}
}

@article{trivedi2021musique,
    title={{M}u{S}i{Q}ue: Multihop Questions via Single-hop Question Composition},
    author={Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
    journal={Transactions of the Association for Computational Linguistics},
    year={2022},
    publisher={MIT Press}
}

@inproceedings{ho-etal-2020-constructing,
    title = "Constructing A Multi-hop {QA} Dataset for Comprehensive Evaluation of Reasoning Steps",
    author = "Ho, Xanh  and
      Duong Nguyen, Anh-Khoa  and
      Sugawara, Saku  and
      Aizawa, Akiko",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.580/",
    doi = "10.18653/v1/2020.coling-main.580",
    pages = "6609--6625",
    abstract = "A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by requiring a model to read multiple paragraphs to answer a given question. However, current datasets do not provide a complete explanation for the reasoning process from the question to the answer. Further, previous studies revealed that many examples in existing multi-hop datasets do not require multi-hop reasoning to answer a question. In this study, we present a new multi-hop QA dataset, called 2WikiMultiHopQA, which uses structured and unstructured data. In our dataset, we introduce the evidence information containing a reasoning path for multi-hop questions. The evidence information has two benefits: (i) providing a comprehensive explanation for predictions and (ii) evaluating the reasoning skills of a model. We carefully design a pipeline and a set of templates when generating a question-answer pair that guarantees the multi-hop steps and the quality of the questions. We also exploit the structured format in Wikidata and use logical rules to create questions that are natural but still require multi-hop reasoning. Through experiments, we demonstrate that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required."
}

@proceedings{regnlp-ws-2025-1,
    title = "Proceedings of the 1st Regulatory NLP Workshop (RegNLP 2025)",
    editor = "Gokhan, Tuba  and
      Wang, Kexin  and
      Gurevych, Iryna  and
      Briscoe, Ted",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.regnlp-1.0/"
}

@article{Zhong_Guo_Gao_Ye_Wang_2024, 
    title={MemoryBank: Enhancing Large Language Models with Long-Term Memory},
    volume={38},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/29946},
    DOI={10.1609/aaai.v38i17.29946},
    abstractNote={Large Language Models (LLMs) have drastically reshaped our interactions with artificial intelligence (AI) systems, showcasing impressive performance across an extensive array of tasks. Despite this, a notable hindrance remains—the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems, psychological counseling, and secretarial assistance. Recognizing the necessity for long-term memory, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user’s personality over time by synthesizing information from previous interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory. This mechanism permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a more human-like memory mechanism and enriched user experience. MemoryBank is versatile in accommodating both closed-source models like ChatGPT and open-source models such as ChatGLM. To validate MemoryBank’s effectiveness, we exemplify its application through the creation of an LLM-based chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned with psychological dialog data, SiliconFriend displays heightened empathy and discernment in its interactions. Experiment involves both qualitative analysis with real-world user dialogs and quantitative analysis with simulated dialogs. In the latter, ChatGPT acts as multiple users with diverse characteristics and generates long-term dialog contexts covering a wide array of topics. The results of our analysis reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong capability for long-term companionship as it can provide emphatic response, recall relevant memories and understand user personality.},
    number={17}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Zhong, Wanjun and Guo, Lianghong and Gao, Qiqi and Ye, He and Wang, Yanlin}, 
    year={2024}, 
    month={Mar.}, 
    pages={19724-19731}
}

@inproceedings{10.1145/3586183.3606763,
    author = {Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
    title = {Generative Agents: Interactive Simulacra of Human Behavior},
    year = {2023},
    isbn = {9798400701320},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3586183.3606763},
    doi = {10.1145/3586183.3606763},
    abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
    booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
    articleno = {2},
    numpages = {22},
    keywords = {Human-AI interaction, agents, generative AI, large language models},
    location = {San Francisco, CA, USA},
    series = {UIST '23}
}

@article{Hatalis_Christou_Myers_Jones_Lambert_Amos-Binks_Dannenhauer_Dannenhauer_2024, 
    title={Memory Matters: The Need to Improve Long-Term Memory in LLM-Agents},
    volume={2},
    url={https://ojs.aaai.org/index.php/AAAI-SS/article/view/27688}, DOI={10.1609/aaaiss.v2i1.27688},
    abstractNote={In this paper, we provide a review of the current efforts to develop LLM agents, which are autonomous agents that leverage large language models. We examine the memory management approaches used in these agents. One crucial aspect of these agents is their long-term memory, which is often implemented using vector databases. We describe how vector databases are utilized to store and retrieve information in LLM agents. Moreover we highlight open problems, such as the separation of different types of memories and the management of memory over the agent’s lifetime. Lastly, we propose several topics for future research to address these challenges and further enhance the capabilities of LLM agents, including the use of metadata in procedural and semantic memory and the integration of external knowledge sources with vector databases.},
    number={1},
    journal={Proceedings of the AAAI Symposium Series}, 
    author={Hatalis, Kostas and Christou, Despina and Myers, Joshua and Jones, Steven and Lambert, Keith and Amos-Binks, Adam and Dannenhauer, Zohreh and Dannenhauer, Dustin}, 
    year={2024}, 
    month={Jan.},
    pages={277-280} 
}

@book{alma991005389998907681,
    author = {Tunstall, Lewis and Werra, Leandro von and Wolf, Thomas},
    address = {Sebastopol, CA},
    booktitle = {Natural language processing with transformers : building language applications with Hugging Face},
    isbn = {9781098136789},
    keywords = {Natural language processing (Computer science) ; Python (Computer program language) ; Machine learning ; Cloud computing},
    language = {eng},
    abstract = {"Since their introduction in 2017, transformers have quickly become the dominant architecture for achieving state-of-the-art results on a variety of natural language processing tasks. If you're a data scientist or coder, this practical book -now revised in full color- shows you how to train and scale these large models using Hugging Face Transformers, a Python-based deep learning library.Transformers have been used to write realistic news stories, improve Google Search queries, and even create chatbots that tell corny jokes. In this guide, authors Lewis Tunstall, Leandro von Werra, and Thomas Wolf, among the creators of Hugging Face Transformers, use a hands-on approach to teach you how transformers work and how to integrate them in your applications. You'll quickly learn a variety of tasks they can help you solve. Build, debug, and optimize transformer models for core NLP tasks, such as text classification, named entity recognition, and question answering; Learn how transformers can be used for cross-lingual transfer learning; Apply transformers in real-world scenarios where labeled data is scarce; Make transformer models efficient for deployment using techniques such as distillation, pruning, and quantization; Train transformers from scratch and learn how to scale to multiple GPUs and distributed environments." -- provided by publisher.},
    publisher = {O'Reilly Media},
    title = {Natural language processing with transformers : building language applications with Hugging Face },
    year = {2023 - 2022},
}

@inproceedings{dai-etal-2019-transformer,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285/",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
    abstract = "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
}

@inproceedings{10.5555/3295222.3295349,
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
    title = {Attention is all you need},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {6000–6010},
    numpages = {11},
    location = {Long Beach, California, USA},
    series = {NIPS'17}
}

@misc{chen2023extendingcontextwindowlarge,
    title={Extending Context Window of Large Language Models via Positional Interpolation}, 
    author={Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},
    year={2023},
    eprint={2306.15595},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2306.15595}, 
}

@inproceedings{ding2024longrope,
    author = {Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao},
    title = {LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens},
    booktitle = {ICML 2024},
    year = {2024},
    month = {February},
    abstract = {Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.},
    url = {https://www.microsoft.com/en-us/research/publication/longrope-extending-llm-context-window-beyond-2-million-tokens/},
}

@inproceedings{wang2023augmentinglanguagemodelslongterm,
    author = {Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
    title = {Augmenting language models with long-term memory},
    year = {2023},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {Existing large language models (LLMs) can only afford fix-sized inputs due to the input length limit, preventing them from utilizing rich long-context information from past inputs. To address this, we propose a framework, Language Models Augmented with Long-Term Memory (LONGMEM), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness. Enhanced with memory-augmented adaptation training, LONGMEM can thus memorize long past context and use long-term memory for language modeling. The proposed memory retrieval module can handle unlimited-length context in its memory bank to benefit various downstream tasks. Typically, LONGMEM can enlarge the long-form memory to 65k tokens and thus cache many-shot extra demonstration examples as long-form memory for in-context learning. Experiments show that our method outperforms strong long-context models on ChapterBreak, a challenging long-context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs. The results demonstrate that the proposed method is effective in helping language models to memorize and utilize long-form contents. Our code is open-sourced at https://aka.ms/LongMem.},
    booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
    articleno = {3259},
    numpages = {14},
    location = {New Orleans, LA, USA},
    series = {NIPS '23}
}

@misc{zeng2024structuralmemoryllmagents,
    title={On the Structural Memory of LLM Agents}, 
    author={Ruihong Zeng and Jinyuan Fang and Siwei Liu and Zaiqiao Meng},
    year={2024},
    eprint={2412.15266},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2412.15266}, 
}

@misc{edge2024localglobalgraphrag,
    author={Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Larson, Jonathan},
    title={From Local to Global: A Graph RAG Approach to Query-Focused Summarization},
    year={2024},
    month={April},
    abstract={The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as "What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a naïve RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2404.16130}
}

@inproceedings{wei2023chainofthoughtpromptingelicitsreasoning,
    author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
    title={Chain-of-thought prompting elicits reasoning in large language models},
    year={2022},
    isbn={9781713871088},
    publisher={Curran Associates Inc.},
    address={Red Hook, NY, USA},
    abstract={We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting.Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
    booktitle={Proceedings of the 36th International Conference on Neural Information Processing Systems},
    articleno={1800},
    numpages={14},
    location={New Orleans, LA, USA},
    series={NIPS '22}
}

@inproceedings{
    yao2023react,
    title={ReAct: Synergizing Reasoning and Acting in Language Models},
    author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik R Narasimhan and Yuan Cao},
    booktitle={The Eleventh International Conference on Learning Representations},
    year={2023},
    url={https://openreview.net/forum?id=WE_vluYUL-X}
}

@inproceedings{sun-etal-2024-pearl,
    title={{PEARL}: Prompting Large Language Models to Plan and Execute Actions Over Long Documents},
    author={Sun, Simeng  and
        Liu, Yang  and
        Wang, Shuohang  and
        Iter, Dan  and
        Zhu, Chenguang  and
        Iyyer, Mohit},
    editor={Graham, Yvette  and
        Purver, Matthew},
    booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month={mar},
    year={2024},
    address={St. Julian{'}s, Malta},
    publisher={Association for Computational Linguistics},
    url={https://aclanthology.org/2024.eacl-long.29/},
    pages={469--486},
    abstract={Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND{\_}EVENT, FIND{\_}RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions that require complex reasoning over long narrative texts. PEARL outperforms zero-shot and chain-of-thought prompting on this dataset, and ablation experiments show that each stage of PEARL is critical to its performance. Overall, PEARL is a first step towards leveraging LLMs to reason over long documents.}
}

@inproceedings{yue2024inferencescalinglongcontextretrieval,
    title={Inference Scaling for Long-Context Retrieval Augmented Generation},
    author={Zhenrui Yue and Honglei Zhuang and Aijun Bai and Kai Hui and Rolf Jagerman and Hansi Zeng and Zhen Qin and Dong Wang and Xuanhui Wang and Michael Bendersky},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=FSjIrOm1vz}
}

@inproceedings{language-agent-tutorial,
    title = "Language Agents: Foundations, Prospects, and Risks",
    author = "Su, Yu and Yang, Diyi and Yao, Shunyu and Yu, Tao",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-tutorials.3",
    pages = "17--24",
}

% ------------------------

% Paper that implements a graph RAG for enterprise use cases%
@misc{rasmussen2025zeptemporalknowledgegraph,
      title={Zep: A Temporal Knowledge Graph Architecture for Agent Memory}, 
      author={Preston Rasmussen and Pavlo Paliychuk and Travis Beauvais and Jack Ryan and Daniel Chalef},
      year={2025},
      eprint={2501.13956},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.13956}, 
}

% React for MHQA %
@inproceedings{trivedi-etal-2023-interleaving,
    title = "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
    author = "Trivedi, Harsh  and
      Balasubramanian, Niranjan  and
      Khot, Tushar  and
      Sabharwal, Ashish",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.557/",
    doi = "10.18653/v1/2023.acl-long.557",
    pages = "10014--10037",
    abstract = "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, \textit{what to retrieve} depends on \textit{what has already been derived}, which in turn may depend on \textit{what was previously retrieved}. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning."
}

%  Limitations of current scores for MHQA evaluation %  
@inproceedings{chen-etal-2019-evaluating,
    title = "Evaluating Question Answering Evaluation",
    author = "Chen, Anthony  and
      Stanovsky, Gabriel  and
      Singh, Sameer  and
      Gardner, Matt",
    editor = "Fisch, Adam  and
      Talmor, Alon  and
      Jia, Robin  and
      Seo, Minjoon  and
      Choi, Eunsol  and
      Chen, Danqi",
    booktitle = "Proceedings of the 2nd Workshop on Machine Reading for Question Answering",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5817/",
    doi = "10.18653/v1/D19-5817",
    pages = "119--124",
    abstract = "As the complexity of question answering (QA) datasets evolve, moving away from restricted formats like span extraction and multiple-choice (MC) to free-form answer generation, it is imperative to understand how well current metrics perform in evaluating QA. This is especially important as existing metrics (BLEU, ROUGE, METEOR, and F1) are computed using n-gram similarity and have a number of well-known drawbacks. In this work, we study the suitability of existing metrics in QA. For generative QA, we show that while current metrics do well on existing datasets, converting multiple-choice datasets into free-response datasets is challenging for current metrics. We also look at span-based QA, where F1 is a reasonable metric. We show that F1 may not be suitable for all extractive QA tasks depending on the answer types. Our study suggests that while current metrics may be suitable for existing QA datasets, they limit the complexity of QA datasets that can be created. This is especially true in the context of free-form QA, where we would like our models to be able to generate more complex and abstractive answers, thus necessitating new metrics that go beyond n-gram based matching. As a step towards a better QA metric, we explore using BERTScore, a recently proposed metric for evaluating translation, for QA. We find that although it fails to provide stronger correlation with human judgements, future work focused on tailoring a BERT-based metric to QA evaluation may prove fruitful."
}

% Other scores for MHQA %
@misc{zhang2020bertscoreevaluatingtextgeneration,
      title={BERTScore: Evaluating Text Generation with BERT}, 
      author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
      year={2020},
      eprint={1904.09675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09675}, 
}

% HippoRAG 2 %
@misc{gutiérrez2025ragmemorynonparametriccontinual,
      title={From RAG to Memory: Non-Parametric Continual Learning for Large Language Models}, 
      author={Bernal Jiménez Gutiérrez and Yiheng Shu and Weijian Qi and Sizhe Zhou and Yu Su},
      year={2025},
      eprint={2502.14802},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.14802}, 
}

% Possible future work analyzing agent based architectures in multilingual settings such as arabic languages %
@article{ABDELAZIM202466,
title = {Multi-Hop Arabic LLM Reasoning in Complex QA},
journal = {Procedia Computer Science},
volume = {244},
pages = {66-75},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.179},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924029806},
author = {Hazem Abdelazim and Tony Begemy and Ahmed Galal and Hala Sedki and Ali Mohamed},
keywords = {Large Language Models, Retrieval Augmented Generation, Arabic NLP},
abstract = {The introduction of Large Language Models (LLMs), and generative AI has significantly transformed the field of natural language processing. These models have exhibited profound reasoning capabilities, marking considerable progress across diverse general knowledge reasoning tasks. Consequently, the deployment of LLMs in domain-specific contexts has become a prime objective for governments and corporations eager to leverage the generative AI revolution. However, the Arabic language has notably lagged in attention and development compared to other languages in this arena. This research endeavors to delve into various facets of Arabic closed-domain question and answering systems that emulate the reasoning requirements of private enterprise data. Our study focuses on the practical deployment of Arabic LLMs in targeted applications, specifically utilizing the ACQAD (Arabic Complex Question Answering Dataset), which exhibits multi-hop reasoning. Different strategies are experimented using Long Context Window (LCW) and Retrieval Augmented Generation (RAG). Results showed that decomposing complex questions using Chain-of-Thought reasoning considerably improved the performance from 75% to 92% using LCW, but at much higher token cost compared to RAG. Trade-of between cost and performance showed that 80% accuracy can be attained using only 30% of the cost using RAG Sentence - level embeddings. Microsoft E5 embedding model is used and OpenAI GPT4-turbo LLM which proved superior reasoning performance compared to other Arabic LLMs}
}

% Other tasks such as textual textbook question answering %
@article{ALAWWAD2025111332,
title = {Enhancing textual textbook question answering with large language models and retrieval augmented generation},
journal = {Pattern Recognition},
volume = {162},
pages = {111332},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111332},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010835},
author = {Hessa A. Alawwad and Areej Alhothali and Usman Naseem and Ali Alkhathlan and Amani Jamal},
keywords = {Natural language processing, Textbook question answering, Retrieval augmented generation, Large language models, Llama-2},
abstract = {Textbook question answering (TQA) is a challenging task in artificial intelligence due to the complex nature of context needed to answer complex questions. Although previous research has improved the task, there are still some limitations in textual TQA, including weak reasoning and inability to capture contextual information in the lengthy context. We propose a framework (PLRTQA) that incorporates the retrieval augmented generation (RAG) technique to handle the “out- of-domain” scenario where concepts are spread across different lessons, and utilize transfer learning to handle the long context and enhance reasoning abilities. Our architecture outperforms the baseline, achieving an accuracy improvement of 4. 12% in the validation set and 9. 84% in the test set for textual multiple-choice questions. While this paper focuses on solving challenges in the textual TQA, It provides a foundation for future work in multimodal TQA where the visual components are integrated to address more complex educational scenarios. Code: https://github.com/hessaAlawwad/PLR-TQA}
}

% Justify other use cases for Question Answering tasks%
@article{ALONSO2024102938,
title = {MedExpQA: Multilingual benchmarking of Large Language Models for Medical Question Answering},
journal = {Artificial Intelligence in Medicine},
volume = {155},
pages = {102938},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102938},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724001805},
author = {Iñigo Alonso and Maite Oronoz and Rodrigo Agerri},
keywords = {Large Language Models, Medical Question Answering, Multilinguality, Retrieval Augmented Generation, Natural Language Processing},
abstract = {Large Language Models (LLMs) have the potential of facilitating the development of Artificial Intelligence technology to assist medical experts for interactive decision support. This potential has been illustrated by the state-of-the-art performance obtained by LLMs in Medical Question Answering, with striking results such as passing marks in licensing medical exams. However, while impressive, the required quality bar for medical applications remains far from being achieved. Currently, LLMs remain challenged by outdated knowledge and by their tendency to generate hallucinated content. Furthermore, most benchmarks to assess medical knowledge lack reference gold explanations which means that it is not possible to evaluate the reasoning of LLMs predictions. Finally, the situation is particularly grim if we consider benchmarking LLMs for languages other than English which remains, as far as we know, a totally neglected topic. In order to address these shortcomings, in this paper we present MedExpQA, the first multilingual benchmark based on medical exams to evaluate LLMs in Medical Question Answering. To the best of our knowledge, MedExpQA includes for the first time reference gold explanations, written by medical doctors, of the correct and incorrect options in the exams. Comprehensive multilingual experimentation using both the gold reference explanations and Retrieval Augmented Generation (RAG) approaches show that performance of LLMs, with best results around 75 accuracy for English, still has large room for improvement, especially for languages other than English, for which accuracy drops 10 points. Therefore, despite using state-of-the-art RAG methods, our results also demonstrate the difficulty of obtaining and integrating readily available medical knowledge that may positively impact results on downstream evaluations for Medical Question Answering. Data, code, and fine-tuned models will be made publicly available.11https://huggingface.co/datasets/HiTZ/MedExpQA.}
}

% More on RAG - Related topics expand some ideas %
@inproceedings{10.1145/3637528.3671470,
author = {Fan, Wenqi and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing},
title = {A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy.uniandes.edu.co/10.1145/3637528.3671470},
doi = {10.1145/3637528.3671470},
abstract = {As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {6491–6501},
numpages = {11},
keywords = {fine-tuning, in-context learning, large language model (llm), pre-training, prompting, retrieval augmented generation (rag)},
location = {Barcelona, Spain},
series = {KDD '24}
}

% Related work: Long context LLMs %
@misc{wang2024leavedocumentbehindbenchmarking,
      title={Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA}, 
      author={Minzheng Wang and Longze Chen and Cheng Fu and Shengyi Liao and Xinghua Zhang and Bingli Wu and Haiyang Yu and Nan Xu and Lei Zhang and Run Luo and Yunshui Li and Min Yang and Fei Huang and Yongbin Li},
      year={2024},
      eprint={2406.17419},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17419}, 
}

% Rag interleaves reasoning through LLM for MQHS %
@inproceedings{10.1145/3626772.3657760,
author = {Yang, Diji and Rao, Jinmeng and Chen, Kezhen and Guo, Xiaoyuan and Zhang, Yawen and Yang, Jie and Zhang, Yi},
title = {IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy.uniandes.edu.co/10.1145/3626772.3657760},
doi = {10.1145/3626772.3657760},
abstract = {Although the Retrieval-Augmented Generation (RAG) paradigms can use external knowledge to enhance and ground the outputs of Large Language Models (LLMs) to mitigate generative hallucinations and static knowledge base problems, they still suffer from limited flexibility in adopting Information Retrieval (IR) systems with varying capabilities, constrained interpretability during the multi-round retrieval process, and a lack of end-to-end optimization. To address these challenges, we propose a novel LLM-centric approach, IM-RAG, that integrates IR systems with LLMs to support multi-round RAG through learning Inner Monologues (IM, i.e., the human inner voice that narrates one's thoughts). During the IM process, the LLM serves as the core reasoning model (i.e., Reasoner ) to either propose queries to collect more information via the Retriever or to provide a final answer based on the conversational context. We also introduce a Refiner that improves the outputs from the Retriever, effectively bridging the gap between the Reasoner and IR modules with varying capabilities and fostering multi-round communications. The entire IM process is optimized via Reinforcement Learning (RL) where a Progress Tracker is incorporated to provide mid-step rewards, and the answer prediction is further separately optimized via Supervised Fine-Tuning (SFT). We conduct extensive experiments with the HotPotQA dataset, a popular benchmark for retrieval-based, multi-step question-answering. The results show that our approach achieves state-of-the-art (SOTA) performance while providing high flexibility in integrating IR modules as well as strong interpretability exhibited in the learned inner monologue.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {730–740},
numpages = {11},
keywords = {inner monologue, large language models, multi-round retrieval, question answering, retrieval augmented generation},
location = {Washington DC, USA},
series = {SIGIR '24}
}