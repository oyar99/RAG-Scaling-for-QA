% Paper that describes an analogy where LLMs can act as the processor and implement MemGPT which for a long time was the state of the art system for memory in many benchmarks %
@misc{packer2024memgptllmsoperatingsystems,
      title={MemGPT: Towards LLMs as Operating Systems}, 
      author={Charles Packer and Sarah Wooders and Kevin Lin and Vivian Fang and Shishir G. Patil and Ion Stoica and Joseph E. Gonzalez},
      year={2024},
      eprint={2310.08560},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.08560}, 
}

% Paper that describes the pipeline used to create a very large dataset of conversations for multi hop QA task% 
@misc{maharana2024evaluatinglongtermconversationalmemory,
      title={Evaluating Very Long-Term Conversational Memory of LLM Agents}, 
      author={Adyasha Maharana and Dong-Ho Lee and Sergey Tulyakov and Mohit Bansal and Francesco Barbieri and Yuwei Fang},
      year={2024},
      eprint={2402.17753},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17753}, 
}

% Paper that presents a naive RAG solution for implementing long term memory for agents%
@misc{zhong2023memorybank,
    title={MemoryBank: Enhancing Large Language Models with Long-Term Memory},
    author={Wanjun Zhong and Lianghong Guo and Qiqi Gao and He Ye and Yanlin Wang},
    year={2023},
    eprint={2305.10250},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

% Paper that introduces many-shot and other prompt strategies for RAG and demonstrates state of the art results% 
@misc{yue2024inferencescalinglongcontextretrieval,
      title={Inference Scaling for Long-Context Retrieval Augmented Generation}, 
      author={Zhenrui Yue and Honglei Zhuang and Aijun Bai and Kai Hui and Rolf Jagerman and Hansi Zeng and Zhen Qin and Dong Wang and Xuanhui Wang and Michael Bendersky},
      year={2024},
      eprint={2410.04343},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.04343}, 
}

% Paper that compares how different storage structures perform with memory benchmarks %
@misc{zeng2024structuralmemoryllmagents,
      title={On the Structural Memory of LLM Agents}, 
      author={Ruihong Zeng and Jinyuan Fang and Siwei Liu and Zaiqiao Meng},
      year={2024},
      eprint={2412.15266},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15266}, 
}

% Paper that implements a graph RAG for enterprise use cases%
@misc{rasmussen2025zeptemporalknowledgegraph,
      title={Zep: A Temporal Knowledge Graph Architecture for Agent Memory}, 
      author={Preston Rasmussen and Pavlo Paliychuk and Travis Beauvais and Jack Ryan and Daniel Chalef},
      year={2025},
      eprint={2501.13956},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.13956}, 
}

% Paper that implements a graph RAG with both semantic and episodic nodes to improve memory benchmarks for agents%
@misc{anokhin2024arigraphlearningknowledgegraph,
      title={AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents}, 
      author={Petr Anokhin and Nikita Semenov and Artyom Sorokin and Dmitry Evseev and Mikhail Burtsev and Evgeny Burnaev},
      year={2024},
      eprint={2407.04363},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.04363}, 
}


% Paper that introduces graph RAG as an alternative to answer global queries about a corpus, rather than local queries that use semantic matching %
@misc{edge2024localglobalgraphrag,
      title={From Local to Global: A Graph RAG Approach to Query-Focused Summarization}, 
      author={Darren Edge and Ha Trinh and Newman Cheng and Joshua Bradley and Alex Chao and Apurva Mody and Steven Truitt and Jonathan Larson},
      year={2024},
      eprint={2404.16130},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.16130}, 
}


%% OTHER

@misc{li2024graphreaderbuildinggraphbasedagent,
      title={GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models}, 
      author={Shilong Li and Yancheng He and Hangyu Guo and Xingyuan Bu and Ge Bai and Jie Liu and Jiaheng Liu and Xingwei Qu and Yangguang Li and Wanli Ouyang and Wenbo Su and Bo Zheng},
      year={2024},
      eprint={2406.14550},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.14550}, 
}

@misc{liu2023lostmiddlelanguagemodels,
      title={Lost in the Middle: How Language Models Use Long Contexts}, 
      author={Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
      year={2023},
      eprint={2307.03172},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.03172}, 
}

@misc{wang2023augmentinglanguagemodelslongterm,
      title={Augmenting Language Models with Long-Term Memory}, 
      author={Weizhi Wang and Li Dong and Hao Cheng and Xiaodong Liu and Xifeng Yan and Jianfeng Gao and Furu Wei},
      year={2023},
      eprint={2306.07174},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.07174}, 
}

@misc{minaee2024largelanguagemodelssurvey,
      title={Large Language Models: A Survey}, 
      author={Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
      year={2024},
      eprint={2402.06196},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.06196}, 
}

@misc{geminiteam2024gemini15unlockingmultimodal,
      title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
      author={Gemini Team and others},
      year={2024},
      eprint={2403.05530},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.05530}, 
}

@misc{kitaev2020reformerefficienttransformer,
      title={Reformer: The Efficient Transformer}, 
      author={Nikita Kitaev and Łukasz Kaiser and Anselm Levskaya},
      year={2020},
      eprint={2001.04451},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.04451}, 
}

@misc{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}

@inproceedings{sun-etal-2024-pearl,
    title = "{PEARL}: Prompting Large Language Models to Plan and Execute Actions Over Long Documents",
    author = "Sun, Simeng  and
      Liu, Yang  and
      Wang, Shuohang  and
      Iter, Dan  and
      Zhu, Chenguang  and
      Iyyer, Mohit",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.29/",
    pages = "469--486",
    abstract = "Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND{\_}EVENT, FIND{\_}RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions that require complex reasoning over long narrative texts. PEARL outperforms zero-shot and chain-of-thought prompting on this dataset, and ablation experiments show that each stage of PEARL is critical to its performance. Overall, PEARL is a first step towards leveraging LLMs to reason over long documents."
}

@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}
