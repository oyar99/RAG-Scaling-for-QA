\cleardoublepage
\chapter{Cognitive Language Agents for Question Answering}
\label{ch:development}
\label{ch:chapter3}

\section{Methodology}
\subsection{Datasets}

To evaluate our language agents and baselines models, we use three  well-known benchmarks for the MHQA task, \textbf{HotpotQA} \cite{yang2018hotpotqa}, \textbf{2WikiMultiHopQA} \cite{ho-etal-2020-constructing}, and \textbf{MuSiQue} \cite{trivedi2021musique}. While \textbf{HotpotQA} has been shown to not require genuine multi-hop reasoning, we include it to support comparison with prior work \cite{trivedi2021musique}.

\noindent All evaluations are conducted on the development sets of these datasets, which offer a sufficient number of samples to yield statistically meaningful results. Following the setup proposed by Bernal et al. \cite{NEURIPS2024_6ddc001d}, we construct a corpus for each dataset by combining the annotated supporting passages with distractor passages. Only answerable questions are retained for our evaluation.

\noindent In addition to the MHQA benchmarks, we evaluate our systems on a lesser-studied dataset \textbf{LoCoMo} designed for conversational QA \cite{maharana-etal-2024-evaluating}. For this work, we select a smaller subset consisting of 10 conversations. The questions include a mix of multi-hop, temporal, open-domain, and single-hop queries. As with the other datasets, non-answerable questions are excluded. Including LoCoMo allows us to test whether the proposed cognitive agent architectures can remain effective in simpler conversational QA scenarios.

\noindent Table \ref{tab:dataset_stats} summarizes key statistics of these datasets. MuSiQue and 2WikiMultiHopQA are the most challenging among the selected datasets. Both were explicitly designed to enforce reasoning and mitigate shortcut-taking by LLMs, making them ideal for evaluating reasoning capabilities of our language agents.

\input{chapter3/datasets_stats}

\subsection{Baselines}
\label{baselines_sec}

We evaluated a range of retrieval systems, including both traditional lexical and semantic retrievers, as well as more advanced methods.

\noindent First, we used a BM25 ranking function with hyperparameters $b = 0.75$ and $k_1 = 0.5$. These values were selected experimentally, considering that passages across all four datasets are relatively short \cite{10.1145/2682862.2682863}. Both  documents and queries were processed through a standard normalization pipeline that includes unigram and bigram generation, stopword removal, Snowball stemming, and other text normalization techniques.

\noindent Second, we evaluated a semantic retriever based on the msmarco-bert-base-dot-v5 model, which encodes text into a 768-dimensional embedding space. This model, trained on question answer pairs from the MS Marco dataset, has demonstrated strong performance on a range of natural language tasks including QA \cite{reimers-2019-sentence-bert}.

\noindent Third, we included ColBERTV2, a semantic retriever that leverages contextual late interaction. Unlike single-vector dense retrievers, ColBERTV2 compares queries and documents at the token level, allowing for finer-grained relevance retrieval \cite{santhanam-etal-2022-colbertv2}.

\noindent To explore retrieval methods that leverage structured knowledge, we evaluated HippoRAG, a recent architecture inspired by neurobiological systems. HippoRAG constructs a knowledge graph by extracting triples from the corpus using an LLM. In our experiments, we used QWen2.5-14B-Instruct \cite{qwen2}, though the architecture is model-agnostic. During inference, HippoRAG employs a semantic retriever to locate relevant nodes in the graph \cite{NEURIPS2024_6ddc001d}, in our case Contriever \cite{lei-etal-2023-unsupervised}. This structured representation enables competitive performance, particularly on more challenging datasets such as MuSiQue.

\noindent To construct a strong baseline for these retrievers, we implemented several standard RAG systems, with carefully designed QA prompt templates to maximize question answering scores. (See Appendix \ref{ch:appendices}). For each system, we varied the number of retrieved passages $k$, from $5$ to $100$, to assess the impact of effective context length on performance. We also tested a full-context variant in which all documents are provided to the model, ensuring that relevant ground-truth passages are always included, even if truncation is needed to fit within the model's context window. Due to computational constraints, questions in the full-context setting are processed in batches of size 8, and the model is required to return structured outputs to locate the answer to each question deterministically. While this multitask scenario is not directly comparable with the RAG baselines, it offers useful insight into how model performance scales as more context is used.

\subsection{Cognitive Language Agents}

We explore several language agent architectures designed to improve QA performance by introducing retrieval and decision-making capabilities at different stages of the pipeline.

\noindent First, we implement a multi-agent architecture in which an specialized retrieval agent determines whether additional documents are needed before forwarding the question to a group of answering agents. Operating independently, this agent decides when sufficient context has been gathered. While prior work used a multi-agent architecture for single-document QA, we extend this idea by integrating it into a RAG pipeline \cite{zhao-etal-2024-longagent}.

\noindent Second, we introduce a re-ranking agent, motivated by findings that LLMs can effectively re-rank retrieved results \cite{sun-etal-2023-chatgpt}. The agent refines the initial retrieval set to ensure that the most relevant information is prioritized before generating a response. To support this process, we evaluate several prompt engineering techniques to guide the agent's re-ranking decisions and assess their impact on performance.

\noindent Third, we propose a hybrid retrieval agent that  selects among multiple retrieval strategies, lexical, semantic, or graph-based, depending on the characteristics of the input query. This builds on ideas from systems like AriGraph and HippoRAG, which primarily rely on a single structured memory \cite{anokhin2024arigraphlearningknowledgegraph}\cite{NEURIPS2024_6ddc001d}. In contrast, our agent is allowed to explore a broader set of retrieval mechanisms.

\noindent Finally, these agent are compared against baseline approaches, providing an analysis of their effectiveness, including discussion on system complexity, performance trade-offs, and resource efficiency.


\subsection{Experimental Setup}

Our experiments used GPT-4o-mini (2024-07-18) with a context window of $128k$ tokens. We accessed the model via the OpenAI API using version 2024-12-01-preview, utilizing both the chat completions and batch processing endpoints for cost efficiency.

\noindent We also report results obtained with QWen2.5-14B-Instruct, which supports a $32K$ token context window \cite{qwen2}. This model was executed using vLLM with 16-bit floating precision on two NVIDIA A40 GPUs, each with 46GB of VRAM \cite{kwon2023efficient}.

\noindent For a few scenarios, we additionally evaluated o3-mini (2025-01-31), which provides a $200k$ token context window. For this model, we selected the medium reasoning effort configuration.

\noindent We also experimented with several open-source models, including Qwen2.5-1.5B-Instruct, Gemma 3-27B, Mistral-Nemo-Instruct-2407. However, these models underperformed on more demanding datasets such as 2Wiki and MuSiQue, and thus their results are not emphasized in our analysis.

\noindent All models were run with temperature set to $0$ to ensure deterministic outputs, and frequency and presence penalties were also set to $0$. The maximum number of completion tokens was set to $500$, which we found sufficient, as most answers are brief. Additionally, we designated the newline character (\textbackslash n) as a stop token, since many models, particularly smaller ones, struggled to adhere to the output formatting instructions and often appended additional commentary.