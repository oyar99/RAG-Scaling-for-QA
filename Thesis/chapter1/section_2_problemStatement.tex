\section{Problem Statement}
\label{sec:problemStatement}

Natural language long-input tasks, such as long-document question answering, remain challenging for LLMs despite advancements in extending their context windows. Issues like \textit{"lost in the middle"} and the prohibitively high computational cost of training long-context models make them an inefficient and expensive solutions for handling long-range information \cite{liu2023lostmiddlelanguagemodels}\cite{kitaev2020reformerefficienttransformer}. To address these challenges, various agent-based architectures have been proposed in the context of cognitive language agents \cite{sumers2024cognitive}\cite{packer2024memgptllmsoperatingsystems}\cite{li2024graphreaderbuildinggraphbasedagent}\cite{anokhin2024arigraphlearningknowledgegraph}\cite{zhao-etal-2024-longagent}. However, how these agents can be effectively utilized to maximize their planning and collaboration capabilities in broader natural language tasks, including Question Answering (QA) and Multi-Hop Question Answering (MHQA), remains largely unexplored.
