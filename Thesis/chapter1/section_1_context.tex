\section{Context}
\label{sec:context}

Large Language Models (LLMs) have consistently demonstrated strong performance in various natural language tasks, enabling applications such as conversational interfaces and question answering \cite{minaee2024largelanguagemodelssurvey}. Recent advancements in hardware and model architectures have led to the development of models capable of handling large context windows, including GPT-3.5 Turbo with $16K$ tokens, GPT-4 with up to $32K$ tokens, and Gemini 1.5 Pro, which remarkably supports a $2M$-token context window
\cite{liu2023lostmiddlelanguagemodels}\cite{openai2024gpt4technicalreport}\cite{geminiteam2024gemini15unlockingmultimodal}. Despite these efforts, certain problems remain challenging, such as long-document question answering. Studies have shown that LLM performance tends to degrade when relevant information appears in the middle of the context window rather than at the beginning or end, a phenomenon commonly referred to as \textit{"lost in the middle"} \cite{liu2023lostmiddlelanguagemodels}. Additionally, naively increasing the context length in Transformer-based architectures incurs prohibitively high computational costs \cite{kitaev2020reformerefficienttransformer}.\\

\noindent To address these limitations, researchers have explored alternative strategies that equip LLMs with long-term memory, enabling them to retrieve and reason about external information. Retrieval Agumented Generation (RAG) has been widely used to incorporate new knowledge into LLMs \cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}. However, naive RAG still struggles to integrate knowledge from multiple sources and effectively reason about retrieved content, limitations that have been extensively documented \cite{NEURIPS2024_6ddc001d}\cite{liang2024kagboostingllmsprofessional}. \\

\noindent From a different perspective, long-term memory systems are being thoroughly studied in the context of \textit{Cognitive Language Agents}, a framework for designing intelligent language agents that leverage LLMs to manage their internal state through three key dimensions: memory, action and decision-making. This framework draws from cognitive science to classify memory into four distinct types: \textit{working memory}, which stores perceptual inputs and active knowledge; \textit{episodic memory}, which retains past experiences; \textit{semantic memory}, which contains factual knowledge about the world; and \textit{procedural memory}, which encodes the rules and functions that govern the agent's interaction with its environment. The agent's internal actions dictate how it engages with these memory components, retrieving information from long-term memory and placing it in working memory for reasoning or storing new information in long-term memory as part of its learning process \cite{sumers2024cognitive}. Many LLM-based systems designed to enhance long-range information retention and reasoning capabilites can be analyzed through this framework, including the following examples. \\

\noindent Memory GPT (MemGPT) is a language agent that manages multiple storage tiers, effectively allowing it to process more input despite its limited context window. The agent maintains procedural memory to encode the rules for reading from and writing to these storage tiers. During reasoning, MemGPT retrieves past interactions using dense retrieval, drawing from either an episodic or semantic database. Retrieved information is then loaded into its working memory \cite{packer2024memgptllmsoperatingsystems}.\\

\noindent GraphReader is a language agent specifically designed for handling long-input tasks using structured data. In particular, it utilizes a graph as its long-term memory. This graph is constructed offline by prompting an LLM to extract entities and their related facts, with each vertex representing an entity and its associated information. Once the graph is built, the system retrieves the most relevant nodes using dense retrieval to serve as entry points for exploration. The agent then navigates the graph using a set of predefined functions, such as searching neighboring nodes. The agent follows a rationale-driven plan to aid in decision-making and autonomously determines when it has gathered sufficient information to generate a response \cite{li2024graphreaderbuildinggraphbasedagent}. \\ 

\noindent Further advances in memory architectures have introduced other data structured approaches that explicitly differentiate between semantic and episodic memory. In particular, AriGraph is an agent whose memory module consists of semantic vertices which represent entities, and semantic edges, which capture relationships between them. Conversely, episodic vertices correspond to temporal events and are linked to all semantic vertices observed at that moment \cite{anokhin2024arigraphlearningknowledgegraph}. While this agent was originally designed for text-based interaction games, its competitive performance on question answering benchmarks suggests that agent-based architectures could be leveraged to implement long-term memory systems for a broader range of natural language processing tasks. \\

\noindent In this work, we present a systematic evaluation of three agent-based architectures, largely inspired by the \textit{Cognitive Language Agent} framework, to implement long-term memory for two natural language long-input tasks. Each system is characterized by its own internal action space, determined primarily by its underlying memory storage mechanism: vectors, graphs, or a novel hybrid system. These agents take a user query as input, reason through the question, and iteratively plan and perform actions supported by their respective systems until they have gathered sufficient information to generate a confident response, effectively leveraging agents' reflection and decision-making abilities to optimize memory utilization.\\




