\section{Context}
\label{sec:context}

Large Language Models (LLMs) have consistently demonstrated strong performance across various natural language tasks, enabling a wide range of applications such as conversational interfaces or long document summarization \cite{minaee2024largelanguagemodelssurvey}. Recent advancements in hardware and model architectures have led to the development of models capable of handling large context windows, such as GPT-3.5 Turbo with $16K$ tokens and Gemini 1.5 Pro, which remarkably supports a context window of $2M$ tokens 
\cite{liu2023lostmiddlelanguagemodels}\cite{geminiteam2024gemini15unlockingmultimodal}. While such extended context windows may enhance a model's ability to solve tasks like multi-hop question answering, studies have shown that model performance tends to degrade when the relevant information is located in the middle of the context window rather than at the beginning or end \cite{liu2023lostmiddlelanguagemodels}. Additionally, naively increasing the context length in transformer based architectures involves a prohibitively increase in computational resources \cite{kitaev2020reformerefficienttransformer}. Alternatively, researchers have explored different strategies to solve tasks that require long input by equipping LLMs with long-term memory capabilities that optimally use the allotted context window length.\\

\noindent Memory GPT (MemGPT) is an LLM agent that manages multiple storage tiers, effectively extending the usable context window beyond what is natively supported by the underlying model. It achieves this through a virtual context management approach, allowing the LLM to use function calls to intelligently read from and write to external data sources, dynamically modifying its current context as needed. MemGPT is evaluated by testing the LLM ability to recall specific details from past interactions in a conversational setting. MemGPT demonstrates enhanced performance compared to baselines models, where LLMs only have access to a summary of past conversations within their context windows \cite{packer2024memgptllmsoperatingsystems}.\\

\noindent More recently, Graph Reader builds upon this idea to develop an LLM agent specifically designed for handling long input tasks. It does so by first constructing a graph that captures detailed information from the input text. The LLM agent then navigates this graph using a set of predefined functions, such as searching neighbor nodes, to retrieve contextually relevant information and formulate a final answer. While this approach shares some similarities with MemGPT, its key distinction lies in leveraging a structured graph representation to preserve and retrieve information more effectively \cite{li2024graphreaderbuildinggraphbasedagent}. 


