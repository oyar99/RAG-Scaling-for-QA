\section{Context}
\label{sec:context}

Large Language Models (LLMs) have consistently demonstrated strong performance across various natural language tasks, enabling a wide range of applications such as conversational interfaces or long document summarization. Recent advancements in hardware and model architectures have led to the development of models capable of handling large context windows, such as GPT-3.5 Turbo with $16K$ tokens and Gemini 1.5 Pro, which remarkably supports a context window of $2M$ tokens. While such extended context windows seemingly enhance a model's ability to solve tasks like multi-hop question answering, studies have shown that model performance tends to degrade when the relevant information is located in the middle of the context window rather than at the beginning or end \cite{liu2023lostmiddlelanguagemodels}. To address this issue, researchers have explored various strategies to improve LLMs performance by incorporating long-term memory capabilities.\\

