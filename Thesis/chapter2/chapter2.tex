\cleardoublepage
\chapter{Related Work}
\label{ch:relatedwork}
\label{ch:chapter2}

\subsection{Long Context LLMs}

Other model architectures have been proposed to extend the effective context window length that use transformer variants with modified attention mechanisms. Alternatively, studies have suggested fine tuning LLMs using positional embeddings. However, these methods are currently in an early stage and incur in high costs during training. \\

\noindent Wang et al suggested a model architecture that implements a residual side net that serves as a memory repository for the fixed LLM \cite{wang2023augmentinglanguagemodelslongterm}.

\subsection{Retrieval Augmented Generation}



% STATE OF THE ART
\input{chapter2/section_1_state_of_the_art}
