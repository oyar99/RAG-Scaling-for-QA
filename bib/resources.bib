@misc{minaee2024largelanguagemodelssurvey,
  title={Large Language Models: A Survey}, 
  author={Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
  year={2024},
  eprint={2402.06196},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2402.06196}, 
}

@article{liu2023lostmiddlelanguagemodels,
  author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  address = {One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA},
  issn = {2307-387X},
  journal = {Transactions of the Association for Computational Linguistics},
  keywords = {Computer science ; Linguistics ; Social sciences ; Technology},
  language = {eng},
  abstract = {AbstractWhile recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
  pages = {157-173},
  publisher = {MIT Press},
  title = {Lost in the Middle: How Language Models Use Long Contexts},
  volume = {12},
  year = {2024},
}

@misc{openai2024gpt4technicalreport,
  title={GPT-4 Technical Report}, 
  author={OpenAI and others},
  year={2024},
  eprint={2303.08774},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2303.08774}, 
}

@misc{geminiteam2024gemini15unlockingmultimodal,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
  author={Gemini Team and others},
  year={2024},
  eprint={2403.05530},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2403.05530}, 
}

@inproceedings{kitaev2020reformerefficienttransformer,
  author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
  title={Reformer: The Efficient Transformer},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020},
  url={https://arxiv.org/abs/2001.04451},
  keywords={attention, locality sensitive hashing, reversible layers}
}

@article{packer2024memgptllmsoperatingsystems,
  title={{MemGPT}: Towards LLMs as Operating Systems},
  author={Packer, Charles and Wooders, Sarah and Lin, Kevin and Fang, Vivian and Patil, Shishir G. and Stoica, Ion and Gonzalez, Joseph E.},
  journal={arXiv preprint arXiv:2310.08560},
  year={2024},
  eprint={2310.08560},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2310.08560},
}

@inproceedings{li2024graphreaderbuildinggraphbasedagent,
  title={{G}raph{R}eader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models},
  author={Li, Shilong  and He, Yancheng  and Guo, Hangyu  and Bu, Xingyuan  and Bai, Ge  and Liu, Jie  and Liu, Jiaheng  and Qu, Xingwei  and Li, Yangguang  and Ouyang, Wanli  and Su, Wenbo  and Zheng, Bo},
  editor={Al-Onaizan, Yaser  and Bansal, Mohit  and Chen, Yun-Nung},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  month={nov},
  year={2024},
  address={Miami, Florida, USA},
  publisher={Association for Computational Linguistics},
  url={https://aclanthology.org/2024.findings-emnlp.746/},
  doi={10.18653/v1/2024.findings-emnlp.746},
  pages={12758--12786},
  abstract={Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. Despite numerous efforts made to optimize LLMs for long contexts, challenges persist in robustly processing long inputs. In this paper, we introduce GraphReader, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. It then invokes a set of predefined functions to read node content and neighbors, facilitating a coarse-to-fine exploration of the graph. Throughout the exploration, the agent continuously records new insights and reflects on current circumstances to optimize the process until it has gathered sufficient information to generate an answer. Experimental results on the LV-Eval dataset reveal that GraphReader using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin. Additionally, our approach demonstrates superior performance on four challenging single-hop and multi-hop benchmarks.}
}

@misc{anokhin2024arigraphlearningknowledgegraph,
  title={AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents}, 
  author={Petr Anokhin and Nikita Semenov and Artyom Sorokin and Dmitry Evseev and Mikhail Burtsev and Evgeny Burnaev},
  year={2024},
  eprint={2407.04363},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2407.04363}, 
}

@inproceedings{maharana-etal-2024-evaluating,
  title={Evaluating Very Long-Term Conversational Memory of {LLM} Agents},
  author={Maharana, Adyasha  and
    Lee, Dong-Ho  and
    Tulyakov, Sergey  and
    Bansal, Mohit  and
    Barbieri, Francesco  and
    Fang, Yuwei},
  editor={Ku, Lun-Wei  and
    Martins, Andre  and
    Srikumar, Vivek},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month={aug},
  year={2024},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  url={https://aclanthology.org/2024.acl-long.747/},
  doi={10.18653/v1/2024.acl-long.747},
  pages={13851--13870},
  abstract={Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 600 turns and 16K tokens on avg., over up to 32 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.}
}

@article{wang2023augmentinglanguagemodelslongterm,
  title={Augmenting Language Models with Long-Term Memory},
  author={Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
  journal={arXiv preprint arXiv:2306.07174},
  year={2023},
  eprint={2306.07174},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2306.07174}, 
}

@inproceedings{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages = {9459--9474},
  publisher = {Curran Associates, Inc.},
  title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
  volume = {33},
  year = {2020}
}

@misc{zeng2024structuralmemoryllmagents,
  title={On the Structural Memory of LLM Agents}, 
  author={Ruihong Zeng and Jinyuan Fang and Siwei Liu and Zaiqiao Meng},
  year={2024},
  eprint={2412.15266},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2412.15266}, 
}

@unpublished{edge2024localglobalgraphrag,
  author={Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Larson, Jonathan},
  title={From Local to Global: A Graph RAG Approach to Query-Focused Summarization},
  year={2024},
  month={April},
  abstract={The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as "What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a naïve RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.},
  url={https://www.microsoft.com/en-us/research/publication/from-local-to-global-a-graph-rag-approach-to-query-focused-summarization/},
}

@inproceedings{wei2023chainofthoughtpromptingelicitsreasoning,
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
  title={Chain-of-thought prompting elicits reasoning in large language models},
  year={2022},
  isbn={9781713871088},
  publisher={Curran Associates Inc.},
  address={Red Hook, NY, USA},
  abstract={We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting.Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  booktitle={Proceedings of the 36th International Conference on Neural Information Processing Systems},
  articleno={1800},
  numpages={14},
  location={New Orleans, LA, USA},
  series={NIPS '22}
}

@inproceedings{sun-etal-2024-pearl,
  title={{PEARL}: Prompting Large Language Models to Plan and Execute Actions Over Long Documents},
  author={Sun, Simeng  and
    Liu, Yang  and
    Wang, Shuohang  and
    Iter, Dan  and
    Zhu, Chenguang  and
    Iyyer, Mohit},
  editor={Graham, Yvette  and
    Purver, Matthew},
  booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month={mar},
  year={2024},
  address={St. Julian{'}s, Malta},
  publisher={Association for Computational Linguistics},
  url={https://aclanthology.org/2024.eacl-long.29/},
  pages={469--486},
  abstract={Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND{\_}EVENT, FIND{\_}RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions that require complex reasoning over long narrative texts. PEARL outperforms zero-shot and chain-of-thought prompting on this dataset, and ablation experiments show that each stage of PEARL is critical to its performance. Overall, PEARL is a first step towards leveraging LLMs to reason over long documents.}
}

@inproceedings{yue2024inferencescalinglongcontextretrieval,
  title={Inference Scaling for Long-Context Retrieval Augmented Generation},
  author={Zhenrui Yue and Honglei Zhuang and Aijun Bai and Kai Hui and Rolf Jagerman and Hansi Zeng and Zhen Qin and Dong Wang and Xuanhui Wang and Michael Bendersky},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=FSjIrOm1vz}
}

@article{Zhong_Guo_Gao_Ye_Wang_2024, 
  title={MemoryBank: Enhancing Large Language Models with Long-Term Memory},
  volume={38},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/29946},
  DOI={10.1609/aaai.v38i17.29946},
  abstractNote={Large Language Models (LLMs) have drastically reshaped our interactions with artificial intelligence (AI) systems, showcasing impressive performance across an extensive array of tasks. Despite this, a notable hindrance remains—the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems, psychological counseling, and secretarial assistance. Recognizing the necessity for long-term memory, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user’s personality over time by synthesizing information from previous interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory. This mechanism permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a more human-like memory mechanism and enriched user experience. MemoryBank is versatile in accommodating both closed-source models like ChatGPT and open-source models such as ChatGLM. To validate MemoryBank’s effectiveness, we exemplify its application through the creation of an LLM-based chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned with psychological dialog data, SiliconFriend displays heightened empathy and discernment in its interactions. Experiment involves both qualitative analysis with real-world user dialogs and quantitative analysis with simulated dialogs. In the latter, ChatGPT acts as multiple users with diverse characteristics and generates long-term dialog contexts covering a wide array of topics. The results of our analysis reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong capability for long-term companionship as it can provide emphatic response, recall relevant memories and understand user personality.},
  number={17}, 
  journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
  author={Zhong, Wanjun and Guo, Lianghong and Gao, Qiqi and Ye, He and Wang, Yanlin}, 
  year={2024}, 
  month={Mar.}, 
  pages={19724-19731}
}