"""Predictor module."""
import json
import os
from typing import Optional
from openai.types.chat.chat_completion import ChatCompletion
from openai.types import Batch
from azure_open_ai.batch import queue_batch_job, wait_for_batch_job_and_save_result
from azure_open_ai.chat_completions import chat_completions
from logger.logger import Logger
from models.agent import Agent
from models.dataset import Dataset
from utils.model_utils import supports_batch, supports_temperature_param
from utils.token_utils import estimate_cost, estimate_num_tokens, get_max_output_tokens, truncate_prompt_if_needed


def predictor(args, dataset: Dataset, agent: Agent) -> None:
    """
    Generates predictions for the given dataset using the specified agent.
    The predictions are generated by indexing the dataset and then using the agent to process it.

    Args:
        args (Namespace): the arguments passed to the script
        dataset (Dataset): the dataset to be processed
        agent (Agent): the agent to use

    Raises:
        ValueError: if the model deployment identifier is not provided
    """
    if args.model is None and not args.noop:
        Logger().error(
            """Model deployment identifier not provided. \
Please provide the model deployment identifier using the -m flag.""")
        raise ValueError("Model deployment identifier not provided")

    _ = dataset.read()
    agent.index(dataset)

    batches: Optional[list[Batch]] = None

    if agent.support_batch:
        # Batch here means that questions are batched together in a single request and
        # batches are sent in a batch request
        batches = batch_question_answering(dataset, agent, args)
    else:
        batches = question_answering(
            dataset, agent, args)

    if batches is not None:
        for i, batch in enumerate(batches):
            Logger().info(
                f"Batch job queued with ID: {batch.id} and status: {batch.status}")

            wait_for_batch_job_and_save_result(batch, get_qa_output_path(str(i)))

# pylint: disable-next=too-many-locals
def question_answering(dataset: Dataset, agent: Agent, args) -> Optional[list[Batch]]:
    """
    Generates predictions for the given dataset using the specified agent.
    The predictions are generated by indexing the dataset and then using the agent to process it.
    """
    questions = dataset.get_questions()

    all_questions = [q for _, question_set in questions.items()
                     for q in question_set]

    notebooks = agent.multiprocessing_reason(
        questions=[q['question'] for q in all_questions])

    results = [({'custom_id': question["question_id"],
                 'question': question['question'],
                 'result': result.get_sources()}, result.get_notes())
               for result, question in zip(notebooks, all_questions)]

    with open(get_retrieval_output_path(), 'w', encoding='utf-8') as f:
        for result_json, _ in results:
            r = json.dumps(result_json)
            f.write(r + '\n')

    if agent.standalone:
        with open(get_qa_output_path(), 'w', encoding='utf-8') as f:
            for result, question in zip(notebooks, all_questions):
                result_json = {
                    "custom_id": question["question_id"],
                    "response": {
                        "body": {
                            "choices": [
                                {
                                    "message": {
                                        "role": "assistant",
                                        "content": result.get_notes()
                                    }
                                }
                            ]
                        }
                    }
                }
                r = json.dumps(result_json)
                f.write(r + '\n')

        return None

    prompts = {}
    for result_json, prompt in results:
        prompts[result_json['custom_id']] = prompt

    guard_job(results, args.model, args.noop)

    open_ai_requests = [
        {
            "custom_id": question["question_id"],
            "method": "POST",
            "url": "/chat/completions",
            "body": {
                "model": args.model,
                "messages": [
                    {"role": "system",
                     "content": truncate_prompt_if_needed(prompts[question["question_id"]], args.model)},
                    {"role": "user", "content": question["question"]}
                ],
                "stop": ["\n"],
                "temperature": default_job_args['temperature'] if supports_temperature_param(args.model) else None,
                "frequency_penalty": default_job_args['frequency_penalty'],
                "presence_penalty": default_job_args['presence_penalty'],
                "max_completion_tokens": 500,
            },
        }
        for question in all_questions
    ]

    if not supports_batch(args.model):
        results = chat_completions([
            {
                "custom_id": open_ai_request['custom_id'],
                **open_ai_request['body']
            }
            for open_ai_request in open_ai_requests
        ])

        chat_completions_to_jsonl(results)

        return None

    batched_jobs = split_jobs(open_ai_requests, 190)

    Logger().info(
        f"Total number of batches: {len(batched_jobs)}.")

    return [queue_batch_job(batch) for batch in batched_jobs if batch is not None]


def batch_question_answering(dataset: Dataset, agent: Agent, args) -> Optional[list[Batch]]:
    """
    Generates predictions for the given dataset using the specified agent.
    """
    questions = dataset.get_questions()

    notebooks = agent.batch_reason([q
                                   for _, question_set in questions.items()
                                   for q in question_set])

    results = [({
        'custom_id': f'{Logger().get_run_id()}-{i}',
        'question': notebook.get_questions(),
        'result': notebook.get_sources()
    }, notebook.get_notes()) for i, notebook in enumerate(notebooks)]

    guard_job(results, args.model, args.noop)

    jobs = [{
            "custom_id": result['custom_id'],
            "method": "POST",
            "url": "/chat/completions",
            "body": {
                "model": args.model,
                "messages": [
                    {"role": "system",
                        "content": context},
                    {"role": "user", "content": result['question']}
                ],
                "temperature": default_job_args['temperature'] if supports_temperature_param(args.model) else None,
                "frequency_penalty": default_job_args['frequency_penalty'],
                "presence_penalty": default_job_args['presence_penalty'],
                "max_completion_tokens": int(get_max_output_tokens(args.model)),
                "response_format": {
                    "type": "json_schema",
                    "json_schema": {
                        "strict": True,
                        "name": "question_answering",
                        "schema": {
                            "type": "object",
                            "properties": {
                                "result": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "question_id": {"type": "string"},
                                            "answer": {"type": "string"},
                                        },
                                        "required": ["question_id", "answer"],
                                        "additionalProperties": False
                                    }
                                }
                            },
                            "required": ["result"],
                            "additionalProperties": False
                        }
                    }
                }
            },
            }
            for (result, context) in results]

    Logger().info(
        f"Total number of jobs: {len(jobs)}.")

    if not supports_batch(args.model):
        results = chat_completions([
            {
                "custom_id": open_ai_request['custom_id'],
                **open_ai_request['body']
            }
            for open_ai_request in jobs
        ])

        chat_completions_to_jsonl(results)

        return None

    # Estimate the size of each job in MBs
    batched_jobs = split_jobs(jobs, 190)

    Logger().info(
        f"Total number of batches: {len(batched_jobs)}.")

    return [queue_batch_job(batch) for batch in batched_jobs]


def split_jobs(jobs: list[dict], max_size: int) -> list[list[dict]]:
    """
    Splits the jobs into smaller batches if the total size exceeds the maximum size.

    Args:
        jobs (list[dict]): the list of jobs
        max_size (int): the maximum size of each batch

    Returns:
        list[list[dict]]: the list of batches
    """
    batched_jobs = []
    current_batch = []
    current_batch_size = 0

    for job in jobs:
        job_size = len(json.dumps(job).encode('utf-8')) / \
            (1024 * 1024)  # Convert bytes to MB
        if current_batch_size + job_size > max_size:
            batched_jobs.append(current_batch)
            current_batch = []
            current_batch_size = 0
        current_batch.append(job)
        current_batch_size += job_size

    if len(current_batch) > 0:
        batched_jobs.append(current_batch)

    return batched_jobs


def chat_completions_to_jsonl(results: list[tuple[ChatCompletion, str]]) -> None:
    """
    Convert the results of the chat completions to JSONL format.

    Args:
        results (list[tuple[dict, str]]): the results of the chat completions
    """
    with open(get_qa_output_path(), 'w', encoding='utf-8') as f:
        for result, custom_id in results:
            r = json.dumps({
                "custom_id": custom_id,
                "response": {
                    "body": {
                        "choices": [
                            {
                                "message": {
                                    "role": "assistant",
                                    "content": result.choices[0].message.content
                                }
                            }
                        ],
                        "usage": {
                            "completion_tokens": result.usage.completion_tokens if result.usage else 0,
                            "prompt_tokens": result.usage.prompt_tokens if result.usage else 0,
                            "total_tokens": result.usage.total_tokens if result.usage else 0
                        }
                    }
                }
            })
            f.write(r + '\n')


def get_qa_output_path(postfix: Optional[str] = None) -> str:
    """
    Get the output path for the batch job results.

    Returns:
        str: the output path
    """
    output_dir = os.path.join(os.path.normpath(
        os.getcwd() + os.sep + os.pardir), 'output' + os.sep + 'qa_jobs')
    name = (f'qa_results_{Logger().get_run_id()}.jsonl'
            if postfix is None else f'qa_results_{Logger().get_run_id()}_{postfix}.jsonl')
    return os.path.join(
        output_dir, name)


def get_retrieval_output_path() -> str:
    """
    Get the output path for the batch job results.

    Returns:
        str: the output path
    """
    output_dir = os.path.join(os.path.normpath(
        os.getcwd() + os.sep + os.pardir), 'output' + os.sep + 'retrieval_jobs')
    return os.path.join(
        output_dir, f'retrieval_results_{Logger().get_run_id()}.jsonl')


def guard_job(results: list[tuple[dict, str]], model: str, stop: bool) -> None:
    """
    Guard the job based on the estimated cost.

    Args:
        results (list[tuple[dict, str]]): the results of the job
        model (str): the deployment model name
        stop (bool): whether to stop the job

    Raises:
        RuntimeError: if the cost exceeds $2.0
    """
    if not isinstance(model, str) or len(model) <= 0:
        raise ValueError(
            "model must be a non-empty string.")

    if stop:
        Logger().error("Returning without queuing job.")
        raise ValueError(
            "Returning without queuing job. Please check the arguments."
        )

    cost = 0.0

    for _, prompt in results:
        token_count = estimate_num_tokens(prompt, model)
        cost += estimate_cost(token_count, model)
        if token_count > 15000:
            Logger().warn(
                "Prompt for question exceeds 15,000 tokens. Truncation is recommended.")

    if cost == 0.0:
        Logger().error("Estimated cost is $0.0. Please review the questions.")
        raise RuntimeError("Program terminated forcefully.")

    if cost > 0.0:
        Logger().info(f"Estimated cost: {cost:.2f}")

    if cost > 0.4:
        Logger().warn(
            "Estimated cost exceeds $0.4. \
Please review the questions and ensure they are not too verbose.")

    if cost > 20.0:
        Logger().error(
            "Cost likely exceeds $20.0. Stopping execution ..."
        )
        raise RuntimeError("Program terminated forcefully.")


default_job_args = {
    'temperature': 0.0,
    'max_completion_tokens': 100,
    'frequency_penalty': 0.0,
    'presence_penalty': 0.0
}
