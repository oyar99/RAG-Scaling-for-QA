"""Predictor module."""
from azure_open_ai.batch_deployment import queue_qa_batch_job, queue_qa_job
from logger.logger import Logger
from models.agent import Agent
from models.dataset import Dataset


def predictor(args, dataset: Dataset, agent: Agent) -> None:
    """
    Generates predictions for the given dataset using the specified agent.
    The predictions are generated by indexing the dataset and then using the agent to process it.

    Args:
        args (Namespace): the arguments passed to the script
        dataset (Dataset): the dataset to be processed
        agent (Agent): the agent to use

    Raises:
        ValueError: if the model deployment identifier is not provided
    """
    if args.model is None and not args.noop:
        Logger().error(
            """Model deployment identifier not provided. \
Please provide the model deployment identifier using the -m flag.""")
        raise ValueError("Model deployment identifier not provided")

    _ = dataset.read()
    agent.index(dataset)

    if agent.support_batch:
        # Batch here refers that all the questions are batched together in a single request
        # instead of sending each question separately
        queue_qa_job(
            args.model, dataset, agent, stop=args.noop)
    else:
        # Batch here refers that each question is sent separately
        # instead of sending all questions together
        queue_qa_batch_job(
            args.model, dataset, agent, stop=args.noop)
